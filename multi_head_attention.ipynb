{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a439addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c81aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E], where \n",
    "    \n",
    "    assume sequence is a sentence\n",
    "    L is the sequence length,      (number of words)\n",
    "    N is the batch size, \n",
    "    E is the embedding dimension.  (number of numbers used for representing the semantic information of each word)\n",
    "    \n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "    \n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    \n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    \n",
    "    L, N, E = input.shape\n",
    "    d_k = E // num_heads   ## query size\n",
    "    \n",
    "    ## split the weight\n",
    "    weights = attn_w[:E, :E], \\\n",
    "               attn_w[E:(E*2), :E], \\\n",
    "               attn_w[(E*2):, :E]\n",
    "    q_w, k_w, v_w = weights\n",
    "    \n",
    "    ## no bias\n",
    "    q_proj = torch.matmul(input, q_w.T)\n",
    "    k_proj = torch.matmul(input, k_w.T)\n",
    "    v_proj = torch.matmul(input, v_w.T)\n",
    "    \n",
    "    def reshape_projection(proj):\n",
    "        return proj.reshape(L, N, num_heads, d_k).permute(2,1,0,3)\n",
    "    \n",
    "    query = reshape_projection(q_proj)\n",
    "    key = reshape_projection(k_proj)\n",
    "    value = reshape_projection(v_proj)\n",
    "    \n",
    "    att = F.softmax(torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) , \n",
    "                    dim = -1)\n",
    "    score = torch.matmul(att, value)\n",
    "    \n",
    "    attention = score.permute(2,1,0,3).reshape(L, N, num_heads * d_k)\n",
    "    output_w = output_w.T\n",
    "    output = torch.matmul(attention, output_w)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922bf5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK 4 4 1\n",
      "OK 4 4 2\n",
      "OK 4 4 4\n",
      "OK 4 8 1\n",
      "OK 4 8 2\n",
      "OK 4 8 4\n",
      "OK 4 16 1\n",
      "OK 4 16 2\n",
      "OK 4 16 4\n",
      "OK 8 4 1\n",
      "OK 8 4 2\n",
      "OK 8 4 4\n",
      "OK 8 8 1\n",
      "OK 8 8 2\n",
      "OK 8 8 4\n",
      "OK 8 16 1\n",
      "OK 8 16 2\n",
      "OK 8 16 4\n",
      "OK 16 4 1\n",
      "OK 16 4 2\n",
      "OK 16 4 4\n",
      "OK 16 8 1\n",
      "OK 16 8 2\n",
      "OK 16 8 4\n",
      "OK 16 16 1\n",
      "OK 16 16 2\n",
      "OK 16 16 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Reset random seed \"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\"\"\" Configuration \"\"\"\n",
    "Ls = [4, 8, 16]\n",
    "N = 1\n",
    "Es = [4, 8, 16]\n",
    "Heads = [1, 2, 4]\n",
    "\n",
    "for L in Ls:\n",
    "    for E in Es:\n",
    "        for num_heads in Heads:\n",
    "            \"\"\" Create weight and input \"\"\"\n",
    "            attn_layer = nn.MultiheadAttention(embed_dim=E, \n",
    "                                         num_heads=num_heads, \n",
    "                                         bias=False)\n",
    "            attn_w, output_w = attn_layer.parameters()\n",
    "            input = torch.randn([L, N, E])   ## input: 3D tensor\n",
    "      \n",
    "            result_torch, _ = attn_layer(input, input, input)\n",
    "            result_yours = multi_head_self_attention(input, \n",
    "                                               num_heads,\n",
    "                                               attn_w,\n",
    "                                               output_w)\n",
    "            assert torch.allclose(result_torch, result_yours, atol=1e-07)\n",
    "            print('OK', L, E, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f84b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
