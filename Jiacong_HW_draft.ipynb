{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Beq_QfgMCyZc"
   },
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E],\n",
    "         L is the sequence length,\n",
    "         N is the batch size,\n",
    "         E is the embedding dimension.\n",
    "\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    length = input.size(0)#L\n",
    "    print(f\"length {length}\")\n",
    "    batch_size = input.size(1)# N\n",
    "    print(f\"batch {batch_size}\")\n",
    "    emb_dim = input.size(-1) # E\n",
    "    print(f\"emb_dim {emb_dim}\")\n",
    "    d_k = emb_dim//num_heads\n",
    "    print(f\"head_dim {d_k}\")\n",
    "\n",
    "    q_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    k_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    v_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    q = q_layer(input)\n",
    "    k = k_layer(input)\n",
    "    v = v_layer(input)\n",
    "    qkv = torch.cat((q,k,v), dim = -1) # shape[4,1,12]\n",
    "    print(qkv.shape)\n",
    "    #qkv = torch.matmul(qkv, attn_w) # shape [4,1,4]\n",
    "    qkv = qkv.reshape(length, batch_size, num_heads, 3*d_k)\n",
    "    print(qkv.shape)\n",
    "    qkv = qkv.permute(2,1,0,3)  #[num_head, batch_num, length, 3*head_dim]\n",
    "    q, k, v = qkv.chunk(3, dim = -1)\n",
    "\n",
    "    print(f\"q shape {q.shape}\")\n",
    "    print(f\"k shape {k.shape}\")\n",
    "    print(f\"v shape {v.shape}\")\n",
    "    '''\n",
    "    q = torch.matmul(q, attn_w[:emb_dim, :emb_dim])\n",
    "    k = torch.matmul(k, attn_w[emb_dim:(emb_dim*2), :emb_dim])\n",
    "    v = torch.matmul(v, attn_w[(emb_dim*2):, :emb_dim])'''\n",
    "\n",
    "\n",
    "    values, attenion = attention(q, k, v)\n",
    "    values = values.reshape(length, batch_size, num_heads * d_k)\n",
    "    final_linear_layer = nn.Linear(emb_dim, emb_dim)\n",
    "    out = torch.matmul(final_linear_layer(values), output_w)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nif6n70kC7N_"
   },
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E],\n",
    "         L is the sequence length,\n",
    "         N is the batch size,\n",
    "         E is the embedding dimension.\n",
    "\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    length = input.size(0)#L\n",
    "    print(f\"length {length}\")\n",
    "    batch_size = input.size(1)# N\n",
    "    print(f\"batch {batch_size}\")\n",
    "    emb_dim = input.size(-1) # E\n",
    "    print(f\"emb_dim {emb_dim}\")\n",
    "    d_k = emb_dim//num_heads\n",
    "    print(f\"head_dim {d_k}\")\n",
    "\n",
    "    '''q_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    k_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    v_layer = nn.Linear(emb_dim,  emb_dim)\n",
    "    q = q_layer(input)\n",
    "    k = k_layer(input)\n",
    "    v = v_layer(input)'''\n",
    "\n",
    "    q,k,v = input, input, input\n",
    "    '''q = torch.matmul(input, attn_w[:emb_dim, :emb_dim])\n",
    "    k = torch.matmul(input, attn_w[emb_dim:(emb_dim*2), :emb_dim])\n",
    "    v = torch.matmul(input, attn_w[(emb_dim*2):, :emb_dim])'''\n",
    "\n",
    "    qkv = torch.cat((q,k,v), dim = -1)\n",
    "    print(qkv.shape) # shape[4,1,12]\n",
    "    qkv = torch.matmul(qkv, attn_w)\n",
    "    print(qkv.shape)\n",
    "    qkv = qkv.reshape(length, batch_size, num_heads, 3*d_k)\n",
    "    print(qkv.shape)\n",
    "    qkv = qkv.permute(2,1,0,3)  #[num_head, batch_num, length, 3*head_dim]\n",
    "    q, k, v = qkv.chunk(3, dim = -1)\n",
    "\n",
    "    print(f\"q shape {q.shape}\")\n",
    "    print(f\"k shape {k.shape}\")\n",
    "    print(f\"v shape {v.shape}\")\n",
    "    '''\n",
    "    q = torch.matmul(q, attn_w[:emb_dim, :emb_dim])\n",
    "    k = torch.matmul(k, attn_w[emb_dim:(emb_dim*2), :emb_dim])\n",
    "    v = torch.matmul(v, attn_w[(emb_dim*2):, :emb_dim])'''\n",
    "\n",
    "\n",
    "    values, attenion = attention(q, k, v)\n",
    "    values = values.reshape(length, batch_size, num_heads * d_k)\n",
    "    out = torch.matmul(values, output_w)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P67NfhQ2C_ZK"
   },
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E],\n",
    "         L is the sequence length,\n",
    "         N is the batch size,\n",
    "         E is the embedding dimension.\n",
    "\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    length = input.size(0)#L\n",
    "    print(f\"length {length}\")\n",
    "    batch_size = input.size(1)# N\n",
    "    print(f\"batch {batch_size}\")\n",
    "    emb_dim = input.size(-1) # E\n",
    "    print(f\"emb_dim {emb_dim}\")\n",
    "    d_k = emb_dim//num_heads\n",
    "    print(f\"head_dim {d_k}\")\n",
    "\n",
    "\n",
    "    q_w = attn_w[:emb_dim, :emb_dim] # size [E,E]\n",
    "    k_w = attn_w[emb_dim:(emb_dim*2), :emb_dim] # size [E,E]\n",
    "    v_w = attn_w[(emb_dim*2):, :emb_dim] # size [E,E]\n",
    "\n",
    "    q = torch.matmul(input, q_w) # [length,batch, dim]\n",
    "    print(f\"q: {q.shape}\")\n",
    "    k = torch.matmul(input, k_w)\n",
    "    v = torch.matmul(input, v_w)\n",
    "\n",
    "    q = q.reshape(length, batch_size, num_heads, d_k) #[length, batch_num, num_head, 3*head_dim]\n",
    "    k = k.reshape(length, batch_size, num_heads, d_k)\n",
    "    v = v.reshape(length, batch_size, num_heads, d_k)\n",
    "\n",
    "    print(f\"q shape {q.shape}\")\n",
    "    print(f\"k shape {k.shape}\")\n",
    "    print(f\"v shape {v.shape}\")\n",
    "\n",
    "    q = q.permute(2,1,0,3)\n",
    "    k = k.permute(2,1,0,3)\n",
    "    v = v.permute(2,1,0,3)\n",
    "\n",
    "    print(f\"q shape {q.shape}\") #[num_head, batch_num, length, head_dim]\n",
    "    print(f\"k shape {k.shape}\")\n",
    "    print(f\"v shape {v.shape}\")\n",
    "\n",
    "    values, attenion = attention(q, k, v)\n",
    "    values = values.permute(2,1,0,3)\n",
    "    values = values.reshape(length, batch_size, num_heads * d_k)\n",
    "    out = torch.matmul(values, output_w)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYXs3G8KDKod"
   },
   "outputs": [],
   "source": [
    "\"\"\" Reset random seed \"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\"\"\" Configuration \"\"\"\n",
    "Ls = [4, 8, 16]\n",
    "N = 1\n",
    "Es = [4, 8, 16]\n",
    "Heads = [1, 2, 4]\n",
    "\n",
    "for L in Ls:\n",
    "  for E in Es:\n",
    "    for num_heads in Heads:\n",
    "      \"\"\" Create weight and input \"\"\"\n",
    "      attn_layer = nn.MultiheadAttention(embed_dim=E,\n",
    "                                         num_heads=num_heads,\n",
    "                                         bias=False)\n",
    "      attn_w, output_w = attn_layer.parameters()\n",
    "      input = torch.randn([L, N, E])\n",
    "\n",
    "      result_torch, _ = attn_layer(input, input, input)\n",
    "      result_yours = multi_head_self_attention(input,\n",
    "                                               num_heads,\n",
    "                                               attn_w,\n",
    "                                               output_w)\n",
    "      '''print(\"torch:\\n\",result_torch)\n",
    "      print(\"your:\\n\",result_yours)\n",
    "      print(\"--------------------------------------------\")'''\n",
    "      assert torch.allclose(result_torch, result_yours, atol=1e-07)#1e-07\n",
    "      print('OK', L, E, num_heads)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
