{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a439addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbf1a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attend(query, key, value):\n",
    "    d_k = query.shape[-1]\n",
    "    x = torch.matmul(query, key.transpose(-2, -1))\n",
    "    x /= d_k ** (1/2)\n",
    "    x = torch.softmax(x, dim=-1)\n",
    "    return torch.matmul(x, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1efe6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(input, attn_w):\n",
    "    E = attn_w.shape[1]\n",
    "    # qkv = torch.matmul(input, attn_w)\n",
    "    w_q = attn_w[:E, :]\n",
    "    w_k = attn_w[E:2*E, :]\n",
    "    w_v = attn_w[2*E:, :]\n",
    "    q = torch.matmul(input, w_q.T)\n",
    "    k = torch.matmul(input, w_k.T)\n",
    "    v = torch.matmul(input, w_v.T)\n",
    "    return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97c81aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E], where L is the sequence length,\n",
    "         N is the batch size, E is the embedding dimension.\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    L, N, E = input.shape\n",
    "    d_k = E // num_heads\n",
    "\n",
    "    q, k, v = get_qkv(input, attn_w)\n",
    "    def split_heads(mat):\n",
    "       mat = mat.reshape(L, N, num_heads, d_k)\n",
    "       return mat.permute(2, 1, 0, 3)\n",
    "    q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "    attention = attend(q, k, v)\n",
    "    attention = attention.permute(2, 1, 0, 3)\n",
    "    attention = attention.reshape(L, N, num_heads * d_k)\n",
    "    output = torch.matmul(attention, output_w.T)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "922bf5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK 4 4 1\n",
      "OK 4 4 2\n",
      "OK 4 4 4\n",
      "OK 4 8 1\n",
      "OK 4 8 2\n",
      "OK 4 8 4\n",
      "OK 4 16 1\n",
      "OK 4 16 2\n",
      "OK 4 16 4\n",
      "OK 8 4 1\n",
      "OK 8 4 2\n",
      "OK 8 4 4\n",
      "OK 8 8 1\n",
      "OK 8 8 2\n",
      "OK 8 8 4\n",
      "OK 8 16 1\n",
      "OK 8 16 2\n",
      "OK 8 16 4\n",
      "OK 16 4 1\n",
      "OK 16 4 2\n",
      "OK 16 4 4\n",
      "OK 16 8 1\n",
      "OK 16 8 2\n",
      "OK 16 8 4\n",
      "OK 16 16 1\n",
      "OK 16 16 2\n",
      "OK 16 16 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Reset random seed \"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\"\"\" Configuration \"\"\"\n",
    "Ls = [4, 8, 16]\n",
    "N = 1\n",
    "Es = [4, 8, 16]\n",
    "Heads = [1, 2, 4]\n",
    "\n",
    "for L in Ls:\n",
    "  for E in Es:\n",
    "    for num_heads in Heads:\n",
    "      \"\"\" Create weight and input \"\"\"\n",
    "      attn_layer = nn.MultiheadAttention(embed_dim=E, \n",
    "                                         num_heads=num_heads, \n",
    "                                         bias=False)\n",
    "      attn_w, output_w = attn_layer.parameters()\n",
    "      input = torch.randn([L, N, E])\n",
    "\n",
    "      # print(input, num_heads, attn_w, output_w)\n",
    "      \n",
    "      result_torch, _ = attn_layer(input, input, input)\n",
    "      result_yours = multi_head_self_attention(input, \n",
    "                                               num_heads,\n",
    "                                               attn_w,\n",
    "                                               output_w)\n",
    "\n",
    "      # print(result_torch)\n",
    "      # print(result_yours)\n",
    "      assert torch.allclose(result_torch, result_yours, atol=1e-07)\n",
    "      print('OK', L, E, num_heads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
