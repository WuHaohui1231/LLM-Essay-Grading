{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a439addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c81aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(query, key, value, dropout=None, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    if mask is not None:\n",
    "        mask.cuda()\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    self_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        self_attn = dropout(self_attn)\n",
    "    return torch.matmul(self_attn, value), self_attn\n",
    "\n",
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E], where L is the sequence length,\n",
    "         N is the batch size, E is the embedding dimension.\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    L,N,E = input.shape\n",
    "    assert (E % num_heads == 0)\n",
    "    d_k = E // num_heads\n",
    "    head = num_heads\n",
    "    d_model = E\n",
    "    \n",
    "    linear_query = nn.Linear(d_model, d_model)\n",
    "    linear_key = nn.Linear(d_model, d_model)\n",
    "    linear_value = nn.Linear(d_model, d_model)\n",
    "    linear_out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    n_batch = N\n",
    "    \n",
    "    query = linear_query(input).view(n_batch, -1, head, d_k).transpose(1, 2)  # [b, 8, 32, 64]\n",
    "    key = linear_key(input).view(n_batch, -1, head, d_k).transpose(1, 2)  # [b, 8, 32, 64]\n",
    "    value = linear_value(input).view(n_batch, -1, head, d_k).transpose(1, 2)  # [b, 8, 32, 64]\n",
    "\n",
    "    x, attn = self_attention(query, key, value)\n",
    "    x = x.transpose(1, 2).contiguous().view(n_batch, -1, head * d_k)\n",
    "\n",
    "    return linear_out(x)\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922bf5c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([L, N, E])\n\u001b[1;32m     20\u001b[0m result_torch, _ \u001b[38;5;241m=\u001b[39m attn_layer(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m result_yours \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mattn_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43moutput_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(result_torch, result_yours, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-07\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m'\u001b[39m, L, E, num_heads)\n",
      "Cell \u001b[0;32mIn [2], line 40\u001b[0m, in \u001b[0;36mmulti_head_self_attention\u001b[0;34m(input, num_heads, attn_w, output_w)\u001b[0m\n\u001b[1;32m     37\u001b[0m key \u001b[38;5;241m=\u001b[39m linear_key(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, head, d_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [b, 8, 32, 64]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m value \u001b[38;5;241m=\u001b[39m linear_value(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, head, d_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [b, 8, 32, 64]\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, head \u001b[38;5;241m*\u001b[39m d_k)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m linear_out(x)\n",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m, in \u001b[0;36mself_attention\u001b[0;34m(query, key, value, dropout, mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mself_attention\u001b[39m(query, key, value, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     d_k \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         mask\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Reset random seed \"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\"\"\" Configuration \"\"\"\n",
    "Ls = [4, 8, 16]\n",
    "N = 1\n",
    "Es = [4, 8, 16]\n",
    "Heads = [1, 2, 4]\n",
    "\n",
    "for L in Ls:\n",
    "  for E in Es:\n",
    "    for num_heads in Heads:\n",
    "      \"\"\" Create weight and input \"\"\"\n",
    "      attn_layer = nn.MultiheadAttention(embed_dim=E, \n",
    "                                         num_heads=num_heads, \n",
    "                                         bias=False)\n",
    "      attn_w, output_w = attn_layer.parameters()\n",
    "      input = torch.randn([L, N, E])\n",
    "      \n",
    "      result_torch, _ = attn_layer(input, input, input)\n",
    "      result_yours = multi_head_self_attention(input, \n",
    "                                               num_heads,\n",
    "                                               attn_w,\n",
    "                                               output_w)\n",
    "      assert torch.allclose(result_torch, result_yours, atol=1e-07)\n",
    "      print('OK', L, E, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f84b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
