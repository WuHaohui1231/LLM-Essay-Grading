{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a439addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c81aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_self_attention(input, num_heads, attn_w, output_w):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Your implementation for the multihead attention function.\n",
    "    We assume the input and the output have the same shape.\n",
    "    input: the input with shape [L, N, E], where L is the sequence length,\n",
    "         N is the batch size, E is the embedding dimension.\n",
    "    num_heads: number of the attention heads each with dimension E // num_heads.\n",
    "    attn_w: the weight for the query, key, and value, with shape [3 * E, E].\n",
    "    output_w: the additional linear layer with shape [E, E].\n",
    "    \"\"\"\n",
    "    L,N,E = input.shape\n",
    "    assert (E % num_heads == 0)\n",
    "    d_k = E // num_heads\n",
    "    \n",
    "    q_w = attn_w[:E,:E]\n",
    "    k_w = attn_w[E:(E*2),:E]\n",
    "    v_w = attn_w[(E*2):,:E] \n",
    "    \n",
    "    linear_query = torch.matmul(input,q_w)\n",
    "    linear_key = torch.matmul(input,k_w)\n",
    "    linear_value = torch.matmul(input,v_w)\n",
    "\n",
    "    \n",
    "   # print(\"batch,head,sequence legth,dk:\",N,num_heads,L,d_k)\n",
    "    \n",
    "    query = linear_query.view(N, L, num_heads, d_k).transpose(1, 2)  # [batch, head, L , dk]\n",
    "    key = linear_key.view(N, L, num_heads, d_k).transpose(1, 2)  \n",
    "    value = linear_value.view(N, L, num_heads, d_k).transpose(1, 2)  \n",
    "    \n",
    "   # print(\"reshape:\",query.shape)\n",
    "    \n",
    "    #self attention\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    self_attn = F.softmax(scores, dim=-1)\n",
    "    x = torch.matmul(self_attn, value)\n",
    "    \n",
    "    x = x.transpose(1, 2).reshape(N, L, E)\n",
    "\n",
    "    return torch.matmul(x,output_w)\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922bf5c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m result_torch, _ \u001b[38;5;241m=\u001b[39m attn_layer(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     21\u001b[0m result_yours \u001b[38;5;241m=\u001b[39m multi_head_self_attention(\u001b[38;5;28minput\u001b[39m, \n\u001b[1;32m     22\u001b[0m                                          num_heads,\n\u001b[1;32m     23\u001b[0m                                          attn_w,\n\u001b[1;32m     24\u001b[0m                                          output_w)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(result_torch, result_yours, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-07\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m'\u001b[39m, L, E, num_heads)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Reset random seed \"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\"\"\" Configuration \"\"\"\n",
    "Ls = [4, 8, 16]\n",
    "N = 1\n",
    "Es = [4, 8, 16]\n",
    "Heads = [1, 2, 4]\n",
    "\n",
    "for L in Ls:\n",
    "  for E in Es:\n",
    "    for num_heads in Heads:\n",
    "      \"\"\" Create weight and input \"\"\"\n",
    "      attn_layer = nn.MultiheadAttention(embed_dim=E, \n",
    "                                         num_heads=num_heads, \n",
    "                                         bias=False)\n",
    "      attn_w, output_w = attn_layer.parameters()\n",
    "      input = torch.randn([L, N, E])\n",
    "      \n",
    "      result_torch, _ = attn_layer(input, input, input)\n",
    "      result_yours = multi_head_self_attention(input, \n",
    "                                               num_heads,\n",
    "                                               attn_w,\n",
    "                                               output_w)\n",
    "      assert torch.allclose(result_torch, result_yours, atol=1e-07)\n",
    "      print('OK', L, E, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f84b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
